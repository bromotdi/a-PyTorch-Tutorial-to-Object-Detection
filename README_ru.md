Это **[PyTorch](https://pytorch.org) руководство по детекции объектов**.

Третье из [серии руководств](https://github.com/sgrvinod/Deep-Tutorials-for-PyTorch). Я пишу о самостоятельной _реализации_ классных моделей с помощью потрясающей библиотеки PyTorch.

Предполагается базовое знание PyTorch и сверточных нейронных сетей.

Если вы новичок в PyTorch, сначала прочитайте [Глубокое обучение с PyTorch: 60-минутный блиц](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) и [Изучение PyTorch на примерах](https:// pytorch.org/tutorials/beginner/pytorch_with_examples.html).

Вопросы, предложения или исправления можно публиковать как issues.

Я использую PyTorch 0.4 в Python 3.6.

---

**4 ноября 2023 г.**: 中文翻译 – китайский перевод этого руководства был любезно предоставлен пользователем [@zigerZZZ](https://github.com/zigerZZZ) – см. [README_zh.md](https:// github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/README_zh.md).

---

# Содержание

[***Цель***](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#objective)

[***Концепции***](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#concepts)

[***Обзор***](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#overview)

[***Реализация***](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#implementation)

[***Обучение***](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#training)

[***Оценка***](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#evaluation)

[***Результаты***](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#inference)

[***Часто задаваемые вопросы***](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#faqs)

# Цель

**Построить модель, которая сможет определять местоположение конкретных объектов на изображениях.**

<p выравнивание="центр">
<img src="./img/baseball.gif">
</p>

Для этой задачи мы будем внедрять [Single Shot Multibox Detector (SSD)](https://arxiv.org/abs/1512.02325), популярную, мощную и особенно гибкую сеть. Оригинальную реализацию авторов можно найти [здесь](https://github.com/weiliu89/caffe/tree/ssd).

Вот несколько примеров детекции объектов на изображениях, которые не использовались во время обучения:

---

<p align="center">
<img src="./img/000001.jpg">
</p>

---

<p align="center">
<img src="./img/000022.jpg">
</p>

---

<p align="center">
<img src="./img/000069.jpg">
</p>

---

<p align="center">
<img src="./img/000082.jpg">
</p>

---

<p align="center">
<img src="./img/000144.jpg">
</p>

---

<p align="center">
<img src="./img/000139.jpg">
</p>

---

<p align="center">
<img src="./img/000116.jpg">
</p>

---

<p align="center">
<img src="./img/000098.jpg">
</p>

---

Дополнительные примеры можно найти в [конце руководства] (https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#some-more-examples).

---

# Концепции

* **Детекция объектов**.

* **Single-Shot Detection (SSD)**. Более ранние архитектуры детекции объектов состояли из двух отдельных этапов: модуля Region Proposal Network (RPN), выполняющего локализацию объектов, и классификатора для определения классов объектов в предлагаемых регионах (proposed regions). С вычислительной точки зрения это может быть очень затратно и поэтому плохо подходит для приложений, работающих в режиме реального времени. Single-shot models объединяют задачи локализации и классификации в одном проходе по сети, что приводит к более быстрой детекции при развертывании на более легком оборудовании.

* **Multiscale Feature Maps (Многомасштабные карты признаков)**. В задачах классификации изображений мы делаем предсказания на основе последней карте признаков после свертки – наименьшего, но наиболее глубокого представления исходного изображения. В детекции объектов карты признаков с промежуточных сверточных слоев также могут быть _непосредственно_ полезны, поскольку они представляют исходное изображение в разных масштабах. Следовательно, фильтр фиксированного размера, работающий на разных картах признаков, сможет обнаружить объекты различных размеров.

* **Priors**. Это заранее рассчитанные боксы, заданные в определенных местах на конкретных картах признаков, с определенными соотношениями сторон и масштабами. Они тщательно выбираются таким образом, чтобы соответствовать характеристикам истинных ограничивающих боксов объектов (т.е. ground truths) в наборе данных.

* **Multibox**. Это [метод] (https://arxiv.org/abs/1312.2249), который формулирует прогнозирование ограничивающей рамки объекта как  _задачу регрессии_, при которой координаты обнаруженного объекта регрессируются к его истинным координатам. Кроме того, для каждого предсказанного бокса генерируются оценки (scores) для различных классов объектов. Priors служат возможными отправными точками для предсказаний, поскольку они моделируются на основе ground truths. Следовательно, количество предсказанных рамок будет равно количеству priors, большинство из которых не содержат объекта.

* **Hard Negative Mining**. Это относится к выбору наиболее явных ложно-положительных срабатываний (False Positive), предсказанных моделью, и принуждению ее учиться на этих примерах. Другими словами, мы анализируем только те ложно-положительные срабатывания, которые модели _труднее_ всего идентифицировать правильно. В контексте детекции объектов, где подавляющее большинство предсказанных боксов не содержит объекта, это также способствует уменьшению дисбаланса между числом ложно-положительных и истинно-положительных срабатываний.

* **Non-Maximum Suppression**. На любом заданном месте несколько priors могут существенно перекрываться. Поэтому предсказания, возникающие из этих priors, могут фактически являться дубликатами одного и того же объекта. Метод Non-Maximum Suppression (NMS) представляет собой способ удаления избыточных предсказаний, подавляя все, кроме того, у которого оценка максимальная.

# Обзор

В этом разделе я представлю обзор данной модели. Если вы уже знакомы с ним, вы можете сразу перейти к разделу [Реализация](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#implementation) или к коду с комментариями.

По мере того, как мы продолжим, вы заметите, что в структуре и формулировке SSD заложено немало инженерных решений. Не волнуйтесь, если некоторые аспекты поначалу покажутся надуманными или не слишком интуитивно понятными. Помните, что они основаны на _многолетних_ исследованиях (часто эмпирических) в данной области.

### Некоторые определения

Box - это просто бокс. _bounding_ box - это ограничивающий бокс, который охватывает объект и представляет его границы.

В этом руководстве мы столкнемся с обеими типами - обычными боксами и ограничивающими боксами. Но для всех боксов, представленных на изображениях, нам нужно уметь измерять их положение, форму, размеры и другие свойства.

#### Граничные координаты

Самый очевидный способ представить бокс — это использовать пиксельные координаты линий `x` и `y`, которые составляют его границы.

![](./img/bc1.PNG)

Граничные координаты бокса — это просто **`(x_min, y_min, x_max, y_max)`**.

Но значения пикселей практически бесполезны, если мы не знаем фактических размеров изображения.
Лучшим способом было бы представить все координаты в их дробной форме.

![](./img/bc2.PNG)

Теперь координаты не зависят от размера, и все боксы на всех изображениях измеряются в одном масштабе.

####  Center-Size coordinates (Координаты центра и размеры)

Это более явный способ представления положения и размеров бокса.

![](./img/cs.PNG)

Координаты центра и размеры бокса представляют собой: **`(c_x, c_y, w, h)`**.

В коде вы обнаружите, что мы обычно используем обе системы координат в зависимости от их пригодности для задачи, но _всегда_ в их дробных формах.

#### Jaccard Index (Коэффициент Жаккара)

Мера Жаккара или Jaccard Overlap или метрика Intersection over Union (IoU) измеряют **меру перекрытия двух боксов**.

![](./img/jaccard.jpg)

Значение IoU, равное `1`, означает, что это один и тот же бокс, а значение `0` указывает, что это взаимоисключающие области.

Это простая метрика, но она находит множество применений в нашей модели.

### Multibox

Multibox — это метод для детекции объектов, где предсказание состоит из двух компонентов:

- **Координаты бокса, который может содержать или не содержать объект**. Это задача _регрессии_.

- **Оценки для различных классов объектов для этого бокса**, включая класс _background_ (фона), который подразумевает, что в боксе нет объектов. Это задача _классификации_.

### Single Shot Detector (SSD)

SSD — это полностью сверточная нейронная сеть (CNN), которую мы можем разделить на три части:

- __Базовые свертки__, полученные из существующей архитектуры классификации изображений, которые предоставят карты признаков более низкого уровня.

- __Вспомогательные свертки__ (__Auxiliary convolutions__), добавленные поверх базовой сети, которые предоставят карты признаков более высокого уровня.

- __Свертки предсказаний__, которые будут находить и идентифицировать объекты на этих картах признаков.

В статье демонстрируются два варианта модели под названием SSD300 и SSD512. Суффиксы представляют размери входного изображения. Хотя эти две сети немного отличаются по способу построения, в принципе они одинаковы. SSD512 — это просто более крупная сеть, обеспечивающая немного лучшую производительность.

Для удобства мы рассмотрим SSD300.

### Базовые свертки – 1 часть

Прежде всего, зачем использовать свертки из существующей архитектуры нейронной сети?

Потому что модели, доказавшие свою эффективность в классификации изображений, уже довольно хорошо улавливают основные черты изображения. Те же самые сверточные слои полезны для детекции объектов, хотя и в более _локальном_ смысле: нас интересуют конкретные области изображения, где присутствуют объекты, а не изображение в целом.

Дополнительным преимуществом является возможность использовать слои, предварительно обученные на основе надежного набора данных для классификации. Как вы, возможно, знаете, это называется **трансферное обучение**. Заимствуя знания из другой, но тесно связанной задачи, мы добились прогресса, даже не начав.

Авторы статьи используют **архитектуру VGG-16** в качестве базовой модели. В ее первоначальной форме она довольно проста.

![](./img/vgg16.PNG)

Они рекомендуют использовать модель, предварительно обученную на задаче классификации _ImageNet Large Scale Visual Recognition Competition (ILSVRC)_. К счастью, такая модель уже доступна в PyTorch, также как и другие популярные архитектуры. Если хотите, вы можете выбрать что-то побольше, например ResNet. Просто помните о вычислительных требованиях.

Согласно статье, **мы должны внести некоторые изменения в эту предварительно обученную сеть**, чтобы адаптировать ее к нашей собственной задаче детекции объектов. Некоторые изменения логичны и необходимы, в то время как другие скорее вопрос удобства или предпочтения.

– **Размер входного изображения** будет равен `300, 300`, как было указано ранее.

- **3-й слой пулинга**, который уменьшает размеры вдвое, будет использовать математическую функцию `ceiling` вместо используемой по умолчанию функции `floor` для определения размера выходного слоя. Это важно только в том случае, если размеры предыдущей карты признаков являются нечетными, а не четными. Глядя на изображение выше, вы можете подсчитать, что для входного изображения размером `300, 300` карта признаков `conv3_3` будет иметь поперечное сечение `75, 75`, которое уменьшит размеры вдвое до `38, 38`, вместо неудобного `37, 37`.

- Мы модифицируем **5-й слой пулинга** с ядром `2, 2` и шагом `2` на ядро `3, 3` и шаг `1`. Это означает, что теперь размеры карты признаков из предыдущего сверточного слоя уже не уменьшатся вдвое.

- Нам не нужны полносвязные (т.е. классификационные) слои, потому что они не несут никакой ценной информации для данной задачи. Мы полностью отбрасываем `fc8`, но решаем **_переделать_ `fc6` и `fc7` в сверточные слои `conv6` и `conv7`**.

Первые три модификации достаточно просты, но последняя, вероятно, требует некоторых объяснений.

### FC → Сверточный слой

Как мы можем переопределить полносвязный слой в сверточный слой?

Рассмотрим следующий сценарий.

В типичной ситуации классификации изображений первый полносвязный слой не может работать _напрямую_ с предыдущей картой признаков или изображением. Мы должны преобразовать его в одномерный вектор.

![](./img/fcconv1.jpg)

В этом примере есть изображение размером `2, 2, 3`, вытянутое в одномерный вектор размером `12`. Чтобы выходное значение было размером `2`, полносвязный слой вычисляет два скалярных произведения между этим вытянутым изображением и двумя векторами того же размера `12`. **Эти два вектора, показанные серым цветом, являются параметрами полносвязного слоя.**

Теперь рассмотрим другой сценарий, в котором мы используем сверточный слой для получения `2`-х выходных значений.

![](./img/fcconv2.jpg)

Здесь изображение размером `2, 2, 3`, очевидно, не нужно выпрямлять. Сверточный слой использует два фильтра, по `12` элементов каждый, той же формы, что и изображение, для выполнения двух скалярных произведений. **Эти два фильтра, показанные серым цветом, являются параметрами сверточного слоя.**

Но вот ключевая часть – **в обоих сценариях выходы `Y_0` и `Y_1` одинаковы!**

![](./img/fcconv3.jpg)

Оба сценария эквивалентны.

Что это нам говорит?

**На изображении размером `H, W` с входными каналами `I` полносвязный слой с выходным размером `N` эквивалентен сверточному слою с размером ядра, равным размеру изображения `H, W` и с `N` выходными каналами**, при условии, что параметры полносвязной сети `N, H * W * I` совпадают с параметрами сверточного слоя `N, H, W, I`.

![](./img/fcconv4.jpg)

Таким образом, любой полносвязанный слой можно преобразовать в эквивалентный сверточный слой, просто **изменив его параметры**.

### Базовые свертки – часть 2

Теперь мы знаем, как преобразовать `fc6` и `fc7` в оригинальной архитектуре VGG-16 в `conv6` и `conv7` соответственно.

В ImageNet VGG-16 [показанной ранее] (https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#base-convolutions--part-1), которая работает с изображениями размером `224, 224, 3`, вы можете видеть, что выходные данные `conv5_3` будут иметь размер `7, 7, 512`. Следовательно –

- `fc6` с размером вытянутого входного вектора `7 * 7 * 512` и размером выходного вектора `4096` имеет параметры размерности `4096, 7 * 7 * 512`. **Эквивалентный сверточный слой `conv6` имеет размер ядра `7, 7` и `4096` выходных каналов, с измененной формой параметров размерности `4096, 7, 7, 512`.**

- `fc7` с входными данными размером `4096` (т. е. с размером выходных данных `fc6`) и выходными данными размером `4096` имеет параметры размеренности `4096, 4096`. Входные данные можно рассматривать как изображение `1, 1` с `4096` входными каналами. **Эквивалентный сверточный слой `conv7` имеет размер ядра `1, 1` и `4096` выходных каналов, с измененной формой размеренности параметров `4096, 1, 1, 4096`.**

Мы видим, что у `conv6` есть `4096` фильтров, каждый размеренности `7, 7, 512`, а у `conv7` есть `4096` фильтров, каждый размеренности `1, 1, 4096`.

Эти фильтры многочисленны, большие и требуют значительных вычислительных затрат.

Чтобы исправить это, авторы решили **уменьшить как количество фильтров, так и размер каждого фильтра, выполнив субдискретизацию (subsampling) параметров** из преобразованных сверточных слоев.

- `conv6` будет использовать `1024` фильтра, каждый размером `3, 3, 512`. Следовательно, параметры субдискретизируются с `4096, 7, 7, 512` до `1024, 3, 3, 512`.

- `conv7` будет использовать `1024` фильтра, каждый размером `1, 1, 1024`. Следовательно, параметры субдискретизируются с `4096, 1, 1, 4096` до `1024, 1, 1, 1024`.

Основываясь на ссылках в статье, мы будем **субдискретизировать, выбирая каждый `m`-й параметр ядра свертки** в процессе, известном как [_децимация_](https://ru.wikipedia.org/wiki/%D0%94%D0%B5%D1%86%D0%B8%D0%BC%D0%B0%D1%86%D0%B8%D1%8F_(%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D1%81%D0%B8%D0%B3%D0%BD%D0%B0%D0%BB%D0%BE%D0%B2)).  

Поскольку ядро `conv6` децимируется с `7, 7` до `3, 3`, сохраняя только каждое третье значение, в ядре теперь есть _пропуски_. Следовательно, нам нужно будет **сделать ядро расширенным (dilated или _atrous_)**.

Это соответствует расширению - dilation - `3` (так же, как коэффициент децимации `m = 3`). Однако на самом деле авторы используют расширение `6`, возможно, потому, что 5-й слой пулинга больше не уменьшает вдвое размеры предыдущей карты признаков.

Теперь мы можем представить нашу базовую сеть - **модифицированную VGG-16**.

![](./img/modifiedvgg.PNG)

На рисунке выше обратите особое внимание на выходные данные `conv4_3` и `conv_7`. Вскоре вы поймете, почему.

### Вспомогательные свертки (Auxiliary Convolutions)

Теперь мы **добавим еще несколько сверточных слоев к нашей базовой сети**. Эти свертки предоставляют дополнительные карты признаков, каждая из которых становится меньше предыдущей.

![](./img/auxconv.jpg)

Мы вводим четыре сверточных блока, каждый из которых состоит из двух слоев. В то время как в базовой сети уменьшение размера карты признаков происходит за счет пулинга, здесь оно обеспечивается `stride=2` в каждом втором слое.

Еще раз обратите внимание на карты признаков из `conv8_2`, `conv9_2`, `conv10_2` и `conv11_2`.

### Отступление

Прежде чем мы перейдем к сверточным слоям предсказаний, мы должны сначала понять, что именно мы предсказываем. Конечно, это объекты и их положения, _но в какой форме?_

Именно здесь мы должны узнать о _priors_ и той решающей роли, которую они играют в SSD.

#### Priors

Предсказания объектов могут быть весьма разнообразными, и я имею в виду не только их тип. Они могут возникнуть в любом месте, иметь любой размер и форму. Заметьте, что мы не должны говорить о _бесконечных_ возможностях того, где и как может появиться объект. Хотя с математической точки зрения это может быть правдой, многие варианты просто маловероятны или неинтересны. Более того, нам не обязательно добиваться того, чтобы боксы были идеальны до пикселя.

По сути, мы можем дискретизировать математическое пространство потенциальных предсказаний всего лишь до _тысячи_ возможностей.

**Priors — это заранее рассчитанные, фиксированные боксы, которые в совокупности представляют эту вселенную вероятных и приблизительных предсказаний**.

Priors выбираются вручную, но тщательно, на основе форм и размеров основных объектов в нашем наборе данных. Размещая эти priors во всех возможных местах на карте объектов, мы также учитываем разнообразие их положений.

Определяя priors, авторы уточняют следующее:

- **они будут применяться к различным картам признаков - низкого и высокого уровня**, а именно тем, которые получаются из `conv4_3`, `conv7`, `conv8_2`, `conv9_2`, `conv10_2` и `conv11_2`. Это те же карты признаков, которые были указаны на рисунках ранее.

- **если prior имеет масштаб `s`, то его площадь равна площади квадрата со стороной `s`**. Самая большая карта признаков, `conv4_3`, будет иметь priors  масштаба `0.1`, т.е. `10%` от размера изображения, тогда как остальные будут иметь priors с масштабами, линейно увеличивающимися от `0.2` до `0.9`. Как видите, карты признаков большего размера имеют priors меньшего масштаба и поэтому идеально подходят для обнаружения объектов меньшего размера.

- **в _каждой_ ячейке карты признаков будут priors с различными соотношениями сторон**. Все карты признаков будут иметь priors с соотношениями сторон `1:1, 2:1, 1:2`. Промежуточные карты признаков `conv7`, `conv8_2` и `conv9_2` _также_ будут иметь priors с соотношениями сторон `3:1, 1:3`. Более того, все карты признаков будут иметь *один дополнительный prior* с соотношением сторон `1:1` и масштабом, равным геометрическому среднему масштабов текущей и последующей карт признаков.

| Источник карты признаков | Размерность карты признаков | Масштаб Prior | Соотношения сторон | Количество Priors в ячейке | Общее количество Priors на этой карте признаков |
| :-----------: | :-----------: | :-----------: | :-----------: | :-----------: | :-----------: |
| `conv4_3`      | 38, 38       | 0.1 | 1:1, 2:1, 1:2 + дополнительный prior | 4 | 5776 |
| `conv7`      | 19, 19       | 0.2 | 1:1, 2:1, 1:2, 3:1, 1:3 + дополнительный prior | 6 | 2166 |
| `conv8_2`      | 10, 10       | 0.375 | 1:1, 2:1, 1:2, 3:1, 1:3 + дополнительный prior | 6 | 600 |
| `conv9_2`      | 5, 5       | 0.55 | 1:1, 2:1, 1:2, 3:1, 1:3 + дополнительный prior | 6 | 150 |
| `conv10_2`      | 3,  3       | 0.725 | 1:1, 2:1, 1:2 + дополнительный prior | 4 | 36 |
| `conv11_2`      | 1, 1       | 0.9 | 1:1, 2:1, 1:2 + дополнительный prior | 4 | 4 |
| **Итого**      |    –    | – | – | – | **8732 priors** |

Всего определено 8732 priors для SSD300!

#### Визуализация Priors

Мы определили priors с точки зрения их _масштабов_ и _соотношений сторон_.

![](./img/wh1.jpg)

Решение этих уравнений дает  размеры prior `w` и `h`.

![](./img/wh2.jpg)

Теперь мы можем нарисовать их на соответствующих картах признаков.

Например, давайте попробуем визуализировать, как будут выглядеть priors в центральной ячейке карты признаков из `conv9_2`.

![](./img/priors1.jpg)

Те же priors существуют и для каждой из остальных ячеек.

![](./img/priors2.jpg)

#### Предсказания по отношению к Priors

[Ранее](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#multibox) мы говорили, что будем использовать регрессию для определения координат ограничивающего бокса объекта. Но тогда priors, конечно же, не могут представлять наши окончательные предсказанные боксы?

Они этого и не делают.

Повторюсь, что priors представляют собой _приблизительно_ возможные предсказания.

Это означает, что **мы используем каждый prior в качестве приблизительной отправной точки, а затем выясняем, насколько его нужно скорректировать для получения более точного предсказания ограничивающего бокса**.

Таким образом, если каждый предсказанный ограничивающий бокс представляет собой небольшое отклонение от prior, и наша цель — вычислить это отклонение, нам нужен способ измерить его или определить количество.

Рассмотрим кошку, ее предсказанный ограничивающий бокс и prior, с помощью которого было сделано предсказание.

![](./img/ecs1.PNG)

Предположим, что они представлены в center-size координатах, с которыми мы знакомы.

Затем -

![](./img/ecs2.PNG)

Это ответ на вопрос, который мы задали в [начале этого раздела](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#a-detour). Учитывая, что каждый prior корректируется для получения более точного предсказания, **эти четыре смещения `(g_c_x, g_c_y, g_w, g_h)` представляют собой форму, в которой мы будем регрессировать координаты ограничивающих боксов**.

Как видите, каждое смещение нормализуется по масштабу соответствующего prior. Это имеет смысл, поскольку определенное смещение будет менее значимым для большего prior, чем для меньшего.

### Свертки предсказаний

Ранее мы выделили и определили priors для шести карт признаков различного масштаба и уровня детализации, а именно для `conv4_3`, `conv7`, `conv8_2`, `conv9_2`, `conv10_2` и `conv11_2`.

Затем, **для _каждого_ prior _в каждой_ ячейке _на каждой_ карте признаков**, мы хотим предсказать –

- **смещение (offsets) `(g_c_x, g_c_y, g_w, g_h)`** для ограничивающего бокса.

- набор оценок **`n_classes`** для ограничивающего бокса, где `n_classes` представляет общее количество классов объектов (включая класс _background_).

Для выполнения этого самым простым способом, **нам нужны два сверточных слоя для каждой карты признаков** –

- сверточный слой  для **предсказания локализации бокса (_localization_)** с ядром `3, 3`, проходящим по каждой ячейке (т. е. с паддингом и шагом `1`) и `4` фильтрами для _каждого_ prior присутствующем в данной ячейке.

   Эти `4` фильтра для prior вычисляют четыре закодированных смещения `(g_c_x, g_c_y, g_w, g_h)` для ограничивающего бокса, предсказанного на основе этого prior.

- сверточный слой **предсказания _класса_** с ядром `3, 3`, проходящим по каждой ячейке (т. е. с паддингом и шагом `1`) и `n_classes` фильтрами для _каждого_ prior присутствующем в данной ячейке.

   Эти `n_classes` фильтры для prior рассчитывают `n_classes` оценок для этого prior.
  
![](./img/predconv1.jpg)

Все наши фильтры применяются с размером ядра `3, 3`.

На самом деле нам не нужны ядра (или фильтры) той же формы, что и priors, потому что разные фильтры научатся делать прогнозы в отношении различных форм priors.

Давайте посмотрим на **выходные данные этих сверток**. Рассмотрим еще раз карту признаков из `conv9_2`.

![](./img/predconv2.jpg)

Выходные данные слоев локализации и предсказания классов показаны синим и желтым цветом соответственно. Вы можете видеть, что поперечный разрез (`5, 5`) остается неизменным.

Что нас действительно интересует, так это _третье_ измерение, то есть количество каналов. Они содержат фактические предсказания.

Если вы **выберете ячейку, _любую_ ячейку, в предсказаниях локализации и развернете ее**, что вы увидите?

![](./img/predconv3.jpg)

Вуаля! Значение каналов в каждой ячейке предсказаний локализации представляют собой закодированные смещения относительно priors в этой ячейке.

Теперь **сделайте то же самое с предсказаниями классов.** Предположим, `n_classes = 3`.

![](./img/predconv4.jpg)

Как и раньше, эти каналы представляют собой оценки классов для priors в этой ячейке.

Теперь, когда мы понимаем, как выглядят предсказания для карты признаков из `conv9_2`, мы можем **придать им более удобную форму.**

![](./img/reshaping1.jpg)

Мы расположили `150` предсказаний последовательно. Человеческому разуму это должно казаться более интуитивным.

Но давайте не будем останавливаться на достигнутом. Мы могли бы сделать то же самое для предсказаний _всех_ слоев и сложить их вместе.

Ранее мы подсчитали, что для нашей модели определено в общей сложности 8732 priors. Таким образом, будет **8732 предсказанных бокса в форме закодированных смещений и 8732 набора оценок классов**.

![](./img/reshaping2.jpg)

**Это окончательный результат для этапа предсказания.** Стопка боксов, если хотите, с оценками того, что в них находится.

Все складывается воедино, не так ли? Если это ваше первое родео по детекции объектов, я думаю, что теперь в конце туннеля виден слабый свет.

### Функция потерь для Multibox

Основываясь на характере наших предсказаний, легко понять, почему нам может понадобиться уникальная функция потерь. Многие из нас раньше рассчитывали лосси в задачах регрессии или классификации, но редко, если вообще когда-нибудь, делали это _одновременно_.

Очевидно, что наш сложенный лосс должен представлять собой **совокупность лоссов от обоих типов предсказаний** – локализации ограничивающих боксов и оценок классов.

Тогда остается ответить на несколько вопросов:

>_Какая функция потерь будет использоваться для регрессированных ограничивающих боксов?_

>_Будем ли мы использовать многоклассовую перекрестную энтропию для оценок классов?_

>_В каком соотношении мы их объединим?_

>_Как мы сопоставляем предсказанные боксы с истинными (ground truths)?_

>_У нас есть 8732 предсказания! Разве большинство из них не будет содержать объектов? Мы их вообще будем рассматривать?_

Уф… Давайте приступим.

#### Сопоставление предсказаний с истинными значениями (ground truths)

Помните, что суть любого алгоритма обучения с учителем заключается в том, чтобы **иметь возможность сопоставлять предсказания с их истинными значениями (ground truths)**. Это сложно, поскольку детекция объектов является более разнообразной задачей, чем обычная задача обучения.

Чтобы модель могла изучить что-либо, нам необходимо сформулировать задачу таким образом, чтобы можно было сравнивать наши предсказания с объектами, действительно присутствующими на изображении.

Priors позволяют нам сделать именно это!

- **Вычислите Коэффициент Жаккара** между 8732 priors и `N` объектами ground truth. Это создаст тензор размера `8732, N`.

- **Сопоставьте** каждый из 8732 priors с объектом, с которым у него наибольшее перекрытие.

- Если prior при сопоставлении с объектом дает **коэффициент Жаккара менее `0,5`**, то нельзя сказать, что он "содержит" объект и, следовательно, является **_отрицательным_ совпадением**. Учитывая, что у нас есть тысячи priors, большинство priors для объекта будут отрицательными.

- С другой стороны, несколько priors фактически **значительно перекрываются (более `0,5`)** с объектом, и можно сказать, что они "содержат" объект. Это **_положительные_ совпадения**.

- Теперь, когда мы **сопоставили каждый из 8732 priors с истинным значением**, мы, по сути, также **сопоставили соответствующие 8732 предсказания с истинными значениями**.

Воспроизведем эту логику на примере.

![](./img/matching1.PNG)

Для удобства мы предположим, что существует всего семь priors, обозначенных красным цветом. Истинные значения выделены желтым цветом — на этом изображении есть три реальных объекта.

Следуя ранее описанным шагам, получим следующие совпадения:

![](./img/matching2.jpg)

Теперь **для каждого prior есть совпадение**, положительное или отрицательное. В более широком смысле, **у каждого предсказания  есть совпадение**, положительное или отрицательное.

Предсказания, положительно соотнесенные с объектом, теперь имеют истинные координаты, которые будут служить **таргетами для локализации**, т. е. для задачи _регрессии_. Естественно, для отрицательных совпадений целевых координат не будет.

Все предсказания имеют истинные лейбли, которые представляет собой либо класс объекта, если совпадение положительное, либо класс _background_, если совпадение отрицательное. Они используются в качестве **таргетов для предсказания классов**, т.е. для задачи _классификации_.

#### Функция потерь для локализации

У нас **нет истинных координат для отрицательных совпадений**. Это вполне логично. Зачем обучать модель рисовать боксы вокруг пустого пространства?

Таким образом, функция потерь для локализации рассчитывается только на основе того, насколько точно мы регрессируем положительно совпадающие предсказанные боксы к соответствующим истинным координатам.

Поскольку мы предсказали боксы локализации в виде смещений `(g_c_x, g_c_y, g_w, g_h)`, нам также необходимо будет соответствующим образом закодировать истинные координаты перед вычислением лоссов.

Функция потерь для локализации — это усредненное значение лосса **Smooth L1** между закодированными смещениями положительно совпадающих боксов локализации и их ground truths.

![](./img/locloss.jpg)

#### Потери достоверности (Confidence loss)

Каждое предсказание, независимо от того, положительное или отрицательное, имеет связанный с ним истинный лейбл. Важно, чтобы модель распознавала как объекты, так и их отсутствие.

Однако, учитывая, что на изображении обычно присутствует лишь несколько объектов, **подавляющее большинство из тысяч сделанных нами предсказаний на самом деле не содержат объектов**. Как сказал бы Уолтер Уайт, _действуйте осторожно_. Если отрицательные совпадения преобладают над положительными, мы получим модель, которая с меньшей вероятностью обнаружит объекты, поскольку чаще всего ее обучают обнаруживать класс _фона_.

Решение может быть очевидным — ограничить количество отрицательных совпадений, которые будут оцениваться функцией потерь. Но как мы выбираем, какие из них исключить?

Почему бы не использовать те случаи, в которых модель _ошибалась_ больше всего? Другими словами, использовать только те предсказания, в которых модели было сложнее всего распознать отсутствие объектов. Этот подход называется **Hard Negative Mining**.

Количество сложных отрицательных примеров (hard negatives), которые мы будем использовать, скажем, `N_hn`, обычно является фиксированным и кратным количеству положительных совпадений для этого изображения. В данном конкретном случае авторы решили использовать в три раза больше сложных отрицательных примеров, т. е. `N_hn = 3 * N_p`. Сложные отрицательные примеры определяются путем вычисления лосса перекрестной энтропии (Cross Entropy - CE) для каждого отрицательно совпадающего предсказания и выбора тех `N_hn`, у которых лосс самый большой.

Тогда confidence loss — это просто сумма значений функции потерь **перекрестной энтропии** среди положительных и сложных отрицательных совпадений.

![](./img/confloss.jpg)

Вы заметите, что confidence loss усредняется по количеству положительных совпадений.

#### Сложенный лосс

**Сложенный лосс от Multibox представляют собой совокупность двух лоссов**, объединенных в соотношении `α`.

![](./img/totalloss.jpg)

В общем, нам не обязательно выбирать значение для `α`. Это может быть обучаемый параметр.

Однако в SSD авторы используют `α = 1`, т.е. просто складывают два лосса. Мы будем придерживаться этого!

### Обработка предсказаний

После обучения модели мы можем применить ее к изображениям. Однако предсказания все еще находятся в необработанной форме — два тензора, содержащие смещения и оценки классов для 8732 priors. Их необходимо будет обработать, чтобы **получить окончательные, понятные человеку, ограничивающие боксы с лейблами.**

Это влечет за собой следующее –

- У нас есть 8732 предсказанных бокса, представленных в виде смещений `(g_c_x, g_c_y, g_w, g_h)` относительно их соответствующих priors. Декодируем их в граничные координаты, которые, на самом деле, можно интерпретировать напрямую.

- Затем для каждого _не фонового_ класса:

   - Извлекаем оценки для этого класса из всех 8732 боксов.

   - Удаляем боксы, которые не соответствуют определенному порогу оценки.

   - Оставшиеся (не удаленные) боксы являются кандидатами для данного класса объекта.

На этом этапе, если бы вы нарисовали эти блоки-кандидаты на исходном изображении, вы бы увидели **множество сильно перекрывающихся боксов, которые явно избыточны**. Это потому, что весьма вероятно, что из тысяч priors, имеющихся в нашем распоряжении, одному и тому же объекту соответствует более одного предсказания.

Для примера, рассмотрим изображение ниже.

![](./img/nms1.PNG)

В этом изображении только три объекта — две собаки и кошка. Но согласно модели, есть _три_ собаки и _две_ кошки.

Имейте в виду, это лайтовый пример. На самом деле все могло быть намного, намного хуже.

Теперь вам может быть очевидно, какие боксы относятся к одному и тому же объекту. Это потому, что ваш разум может осознать, что определенные боксы существенно совпадают друг с другом и с конкретным объектом.

Но как это сделать на практике?

Во-первых, **выстраиваем кандидатов для каждого класса в зависимости от их _вероятности_**.

![](./img/nms2.PNG)

Мы отсортировали кандидатов по их оценкам.

Следующий шаг – выяснить, какие кандидаты являются избыточными. В нашем распоряжении уже есть инструмент для определения того, насколько два бокса схожи между собой – коэффициент Жаккара.

Итак, если бы нам нужно было **определить меру сходства Жаккара между всеми кандидатами в данном классе**, мы могли бы оценить каждую пару и **если обнаружится, что они значительно перекрываются, оставить только _более вероятного_ кандидата**.

![](./img/nms3.jpg)

Таким образом, мы исключили кандидатов-изгоев – по одному для каждого животного.

Этот процесс называется __подавление не-максимумов (Non-Maximum Suppression (NMS)__, потому что, когда обнаруживается, что несколько кандидатов значительно перекрываются друг с другом и могут ссылаться на один и тот же объект, **мы подавляем всех, кроме одного с максимальной оценкой**.

Алгоритмически это осуществляется следующим образом:

- При выборе кандидатов для каждого _не фонового_ класса,

   - Упорядочиваем кандидатов для этого класса в порядке убывания вероятности.

   - Рассматриваем кандидата с наивысшей оценкой. Исключаем всех кандидатов с более низкими оценками, у которых коэффициент Жаккара с этим кандидатом превышает, скажем, `0.5`.
     
   - Рассматриваем следующего кандидата с наивысшей оценкой, который все еще остается в выборке. Исключаем всех кандидатов с более низкими оценками, у которых коэффициент Жаккара с этим кандидатом превышает `0.5`.

   - Повторяем действия, пока не пройдем по всей выборке кандидатов.

В конечном итоге у нас будет только один бокс – самый лучший – для каждого объекта на изображении.

![](./img/nms4.PNG)

Non-Maximum Suppression (NMS) является весьма важным этапом для получения качественной детекции.

К счастью, это также последний этап.

# Реализация

В разделах ниже кратко описывается реализация.

Они предназначены для предоставления некоторого контекста, но **детали лучше всего понимать непосредственно из кода**, который довольно подробно прокомментирован.

### Набор данных

Мы будем использовать данные Pascal Visual Object Classes (VOC) за 2007 и 2012 годы.

#### Описание

Эти данные содержат изображения с двадцатью различными классами объектов.

```python
{'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'}
```

Каждое изображение может содержать один или несколько ground truth объектов.

Каждый объект представлен:

- ограничивающим боксом в абсолютных граничных координатах

- лейблом (один из классов объектов, упомянутых выше)

- предполагаемой сложностю детекции (либо `0`, что означает _несложно_, либо `1`, что означает _сложно_)

#### Скачать

В частности, вам  нужно скачать следующие наборы данных VOC (Visual Object Classes):

- [2007 _trainval_](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar) (460MB)

- [2012 _trainval_](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar) (2GB)

- [2007 _test_](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar) (451MB)

Согласно статье, два набора данных _trainval_ будут использоваться для обучения, а VOC 2007 _test_ будет служить нашими тестовыми данными.

Убедитесь, что вы извлекли данные VOC 2007 _trainval_ и 2007 _test_ в одну и ту же директорию, т. е. объединили их.

### Входные данные для модели

Нам понадобятся три типа входных данных.

#### Изображения

Поскольку мы используем вариант SSD300, изображения должны иметь размер `300, 300` пикселей и быть в RGB формате.

Помните, что мы используем предварительно обученную на ImageNet базовую модель VGG-16, которая уже доступна в модуле `torchvision` для PyTorch. [На этой странице](https://pytorch.org/docs/master/torchvision/models.html) приведены подробности предварительной обработки или преобразований, которые нам необходимо выполнить, чтобы использовать эту модель — значения пикселей должны находиться в диапазоне [0, 1], а затем мы должны нормализовать изображение по среднему и стандартному отклонению RGB-каналов изображений ImageNet.

```python
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]
```

Кроме того, PyTorch следует соглашению NCHW (батч N, канали C, высота H, ширина W), что означает, что количество каналов (C) должено предшествовать размерам изображения.

Следовательно, **изображения, поступающие на вход модели, должны быть тензорами типа `Float` с размерностями `N, 3, 300, 300`** и их необходимо нормализировать по вышеупомянутому среднему значению и стандартному отклонению. `N` — размер батча.

#### Ограничительные боксы объектов

Нам нужно будет предоставить для каждого изображения ограничивающие боксы истинных объектов, присутствующих в нем, в дробных граничных координатах `(x_min, y_min, x_max, y_max)`.

Поскольку количество объектов на любом изображении может варьироваться, мы не можем использовать тензор фиксированного размера для хранения ограничивающих боксов для всего батча из `N` изображений.

Следовательно, **ограничивающие боксы истинных объектов, поступающие на вход модели, должны быть списком длины `N`, где каждый элемент списка представляет собой тензор типа `Float` с размерностью `N_o, 4`**, где `N_o` — это количество объектов, присутствующих на данном изображении.

#### Лейбли объектов

Необходимо предоставить для каждого изображения лейбли истинных объектов, присутствующих в нем.

Каждый лейбл должен быть закодирован целым числом от `1` до `20`, представляющим двадцать различных классов объектов. Кроме того, мы добавим класс _фона_ с индексом `0`, который указывает на отсутствие объекта в ограничивающем боксе. (Но, естественно, этот лейбл фактически не будет использоваться ни для одного из истинных объектов в наборе данных.)

Опять же, поскольку количество объектов на любом изображении может варьироваться, мы не можем использовать тензор фиксированного размера для хранения лейблов для всего батча из `N` изображений.

Следовательно, **лейбли ground truth, передаваемые в модель, должны представлять собой список длины `N`, где каждый элемент списка представляет собой `Long` тензор с размерностью `N_o`**, где `N_o` — количество объектов на данном изображении.

### Пайплайн обработки данных

Как вы знаете, наши данные разделены на _обучающие_ и _тестовые_ наборы.

#### Обработка исходных данных

Посмотрите на `create_data_lists()` в [`utils.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py).

Эта функция разбирает скачанные данные и сохраняет следующие файлы:

– **JSON-файл для каждого набора данных с перечнем абсолютных путей к файлам `I` изображений**, где `I` — общее количество изображений в наборе.

- **JSON-файл для каждого набора со списком словарей `I`, содержащих истинные объекты, т. е. ограничивающие боксы в абсолютных граничных координатах, их закодированные лейбли и предполагаемые сложности детекции обьектов**. Словарь с индексом `i` в этом перечне будет содержать объекты, присутствующие на `i`-том изображении в предыдущем JSON-файле.

- **JSON-файл, содержащий `label_map`**, словарь сопоставления лейблов и индексов, с помощью которого лейбли кодируются в предыдущем файле JSON. Этот словарь также доступен в [`utils.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py) и его можно напрямую импортировать.

#### PyTorch Dataset

Посмотрите на класс `PascalVOCDataset` в файле [`datasets.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/datasets.py).

Этот класс является подклассом PyTorch [`Dataset`](https://pytorch.org/docs/master/data.html#torch.utils.data.Dataset) и используется для **определения наших обучающих и тестовых наборов данных.** В нем определены метод `__len__`, который возвращает размер набора данных, и метод `__getitem__`, который возвращает `i`-е изображение, ограничивающие боксы и лейбли для объектов на этом изображении, используя ранее сохраненные JSON-файлы.

Вы заметите, что он также возвращает предполагаемые сложности для детекции каждого из этих объектов, но на самом деле они не используются при обучении модели. Они необходимы только на этапе [Оценки](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#evaluation) для вычисления показателя усреднённой средней точности (mean Average Precision - mAP). У нас также есть возможность полностью отфильтровать _сложные_ объекты из наших данных, чтобы ускорить обучение за счет потери некоторой точности.

Кроме того, внутри этого класса **каждое изображение и объекты в них подвергаются целому ряду трансформаций**, как описано в статье и приведено ниже.

#### Преобразование данных

Посмотрите на `transform()` в [`utils.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py).

Эта функция применяет следующие преобразования к изображениям и объектам в них:

- Случайным образом **изменяется яркость, контрастность, насыщенность и оттенок**, каждый каждый с 50% вероятностью и в случайном порядке.

- С вероятностью 50 % **выполняется операция _уменьшения_ изображения.** Это помогает научиться детектировать мелкие объекты. Уменьшенное изображение должно быть от `1` до `4` раз больше оригинала. Окружающее пространство может быть заполнено средним значением данных ImageNet.

- Случайным образом изображение обрезается, т. е. **выполняется операция _увеличения_ изображения.** Это помогает научиться обнаруживать большие или обрезаные объекты. Некоторые объекты могут даже быть полностью вырезаны. Размеры обрезки должны быть в пределах от `0.3` до `1` раза от исходных размеров. Соотношение сторон должно находиться в диапазоне от `0.5` до `2`. Каждая обрезка сделана так, чтобы оставался хотя бы один ограничивающий бокс с коэффициентом Жаккара равным `0`, `0.1`, `0.3`, `0.5`, `0.7` или `0.9`, выбранный случайным образом. Кроме того, все оставшиеся ограничивающие боксы, центры которых больше не находятся на изображении в результате обрезки, отбрасываются. Также есть вероятность того, что изображение вообще не обрезано.

- С вероятностью 50% изображение **отображается по горизонтали**.

- **Изменяется размер** изображения до `300, 300` пикселей. Это требование SSD300.

- Координаты всех боксов преобразовуются из **абсолютных в дробные.** На всех этапах нашей модели все координаты размеров и центра будут иметь дробную форму.

- Изображение **нормализовуется** с помощью среднего и стандартного отклонения данных ImageNet, которые использовались для предварительного обучения нашей базовой модели VGG.

Как отмечено в статье, эти преобразования играют решающую роль в достижении заявленных результатов.

#### PyTorch DataLoader

Описанный выше набор данных, `PascalVOCDataset`, будет использоваться PyTorch [`DataLoader`](https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) в файле `train .py` для **создания и передачи пакетов данных в модель** для обучения или оценки.

Поскольку количество объектов на разных изображениях варьируется, их ограничивающие боксы, лейблы и сложность детекции нельзя просто сложить вместе в батче. Нельзя было бы определить, какие объекты какому изображению принадлежат.

Вместо этого нам нужно **передать функцию слияния (collating function) аргументу `collate_fn`**, которая указывает `DataLoader`, каким образом следует комбинировать эти тензоры разного размера. Самый простой вариант — использовать списки Python.

### Базовые свертки

Посмотрите на `VGGBase` в [`model.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py).

Здесь мы **создаем и применяем базовые свертки.**

Слои инициализируются параметрами предварительно обученной VGG-16 с помощью метода `load_pretrained_layers()`.

Нас особенно интересуют карты признаков более низкого уровня, полученные из `conv4_3` и `conv7`, которые мы возвращаем для использования на последующих этапах.

### Вспомогательные свертки (Auxiliary Convolutions)

Посмотрите на `AuxiliaryConvolutions` в [`model.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py).

Здесь мы **создаем и применяем вспомогательные свертки.**

Мы используем [равномерную инициализацию Ксавье](https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_) для параметров этих слоев.

Нас особенно интересуют карты признаков более высокого уровня, полученные из `conv8_2`, `conv9_2`, `conv10_2` и `conv11_2`, которые мы возвращаем для использования на последующих этапах.

### Свертки предсказаний

Посмотрите на `PredictionConvolutions` в [`model.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py).

Здесь мы **создаем и применяем свертки для локализации и предсказаний классов** к картам признаков из `conv4_3`, `conv7`, `conv8_2`, `conv9_2`, `conv10_2` и `conv11_2`.

Эти слои инициализируются аналогично вспомогательным сверткам.

Мы также **изменяем форму полученных карт предсказаний и объединяем их**, как обсуждалось ранее. Обратите внимание, что изменение формы в PyTorch возможно только в том случае, если исходный тензор хранится в [последовательном](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contigious) блоке памяти.

Как и ожидалось, объединенные предсказания локализации и класса будут иметь размеры `8732, 4` и `8732, 21` соответственно.

### Собираем все вместе

Посмотрите на `SSD300` в [`model.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py).

Здесь **базовые, вспомогательные и прогнозирующие свертки объединяются** для формирования SSD.

Небольшая деталь: ожидается, что признаки самого низкого уровня, то есть признаки из `conv4_3`, будут находиться в значительно другом числовом масштабе по сравнению с их аналогами более высокого уровня. Поэтому авторы сначала рекомендуют выполнить L2-нормализацию, а затем масштабировать _каждый_ его канал на обучаемое значение.

### Priors

Посмотрите на `create_prior_boxes()` внутри класса `SSD300` в [`model.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py).

Эта функция **создает priors в center-size координатах** в соответствии с определенными требованиями для карт признаков из `conv4_3`, `conv7`, `conv8_2`, `conv9_2`, `conv10_2` и `conv11_2`, _в указанном порядке_. Кроме того, для каждой карты признаков мы создаем priors в каждой ячейки, проходя по ним построчно.

Полученный таким образом порядок 8732 priors очень важен, поскольку он должен соответствовать порядку объединенных предсказаний.

### Multibox Loss

Посмотрите на `MultiBoxLoss` в [`model.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py).

Два пустых тензора создаются для хранения таргетов локализации и предсказаний классов, то есть _ground truths_, для 8732 предсказанных боксов на каждом изображении.

Мы **находим истинный объект с максимальным коэффициентом Жаккара для каждого prior**, который хранится в `object_for_each_prior`.

Мы хотим избежать редкой ситуации, когда не все объекты ground truth были сопоставлены. Поэтому мы также **находим prior с максимальным перекрытием для каждого объекта ground truth**, хранящийся в `prior_for_each_object`. Мы явно добавляем эти совпадения в `object_for_each_prior` и искусственно устанавливаем для их перекрытия значение выше порогового значения, чтобы они не были исключены.

На основе совпадений в `object_for_each_prior` мы устанавливаем соответствующие лейблы, то есть **таргеты для предсказаний класса**, каждому из 8732 priors. Для тех priors, которые незначительно перекрываются с соответствующими им объектами, лейбл устанавливается на _фон_.

Кроме того, мы кодируем координаты 8732 совпавших объектов в `object_for_each_prior` в форме смещения `(g_c_x, g_c_y, g_w, g_h)` относительно этих priors, чтобы сформировать **таргеты для задачи локализации**. Не все из этих 8732 таргетов локализации имеют смысл. Как мы обсуждали ранее, только предсказания, возникающие из нефоновых priors, будут регрессированы к таргету.

**Функция потерь для задачи локализации** - это [Smooth L1 loss](https://pytorch.org/docs/stable/nn.html#torch.nn.SmoothL1Loss) по положительным совпадениям.

Perform Hard Negative Mining – rank class predictions matched to _background_, i.e. negative matches, by their individual [Cross Entropy losses](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss). The **confidence loss** is the Cross Entropy loss over the positive matches and the hardest negative matches. Nevertheless, it is averaged only by the number of positive matches.

The **Multibox Loss is the aggregate of these two losses**, combined in the ratio `α`. In our case, they are simply being added because `α = 1`.

# Training

Before you begin, make sure to save the required data files for training and evaluation. To do this, run the contents of [`create_data_lists.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/create_data_lists.py) after pointing it to the `VOC2007` and `VOC2012` folders in your [downloaded data](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#download).

See [`train.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/train.py).

The parameters for the model (and training it) are at the beginning of the file, so you can easily check or modify them should you need to.

To **train your model from scratch**, run this file –

`python train.py`

To **resume training at a checkpoint**, point to the corresponding file with the `checkpoint` parameter at the beginning of the code.

### Remarks

In the paper, they recommend using **Stochastic Gradient Descent** in batches of `32` images, with an initial learning rate of `1e−3`, momentum of `0.9`, and `5e-4` weight decay.

I ended up using a batch size of `8` images for increased stability. If you find that your gradients are exploding, you could reduce the batch size, like I did, or clip gradients.

The authors also doubled the learning rate for bias parameters. As you can see in the code, this is easy do in PyTorch, by passing [separate groups of parameters](https://pytorch.org/docs/stable/optim.html#per-parameter-options) to the `params` argument of its [SGD optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD).

The paper recommends training for 80000 iterations at the initial learning rate. Then, it is decayed by 90% (i.e. to a tenth) for an additional 20000 iterations, _twice_. With the paper's batch size of `32`, this means that the learning rate is decayed by 90% once after the 154th epoch and once more after the 193th epoch, and training is stopped after 232 epochs. I followed this schedule.

On a TitanX (Pascal), each epoch of training required about 6 minutes.

I should note here that two unintended differences from the paper were brought to my attention by readers of this tutorial:

- My priors that overshoot the edges of the image are not being clipped, as pointed out in issue [#94](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/issues/94) by _@AakiraOtok_. This does not appear to have a negative effect on performance, however, as discussed in that issue and also verified in issue [#95](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/issues/95) by the same reader. It is even possible that there is a slight improvement in performance, but this may be too small to be conclusive.

- I mistakenly used L1 loss instead of *smooth* L1 loss as the localization loss, as pointed out in issue [#60](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/issues/60) by _jonathan016_. This also appears to have no negative effect on performance as pointed out in that issue, but _smooth_ L1 loss may offer better training stability with larger batch sizes as mentioned in [this comment](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/issues/94#issuecomment-1590217018). 

### Model checkpoint

You can download this pretrained model [here](https://drive.google.com/open?id=1bvJfF6r_zYl2xZEpYXxgb7jLQHFZ01Qe).

Note that this checkpoint should be [loaded directly with PyTorch](https://pytorch.org/docs/stable/torch.html?#torch.load) for evaluation or inference – see below.

# Evaluation

See [`eval.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/eval.py).

The data-loading and checkpoint parameters for evaluating the model are at the beginning of the file, so you can easily check or modify them should you wish to.

To begin evaluation, simply run the `evaluate()` function with the data-loader and model checkpoint. **Raw predictions for each image in the test set are obtained and parsed** with the checkpoint's `detect_objects()` method, which implements [this process](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#processing-predictions). Evaluation has to be done at a `min_score` of `0.01`, an NMS `max_overlap` of `0.45`, and `top_k` of `200` to allow fair comparision of results with the paper and other implementations.

**Parsed predictions are evaluated against the ground truth objects.** The evaluation metric is the _Mean Average Precision (mAP)_. If you're not familiar with this metric, [here's a great explanation](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173).

We will use `calculate_mAP()` in [`utils.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py) for this purpose. As is the norm, we will ignore _difficult_ detections in the mAP calculation. But nevertheless, it is important to include them from the evaluation dataset because if the model does detect an object that is considered to be _difficult_, it must not be counted as a false positive.

The model scores **77.2 mAP**, same as the result reported in the paper.

Class-wise average precisions (not scaled to 100) are listed below.

| Class | Average Precision |
| :-----: | :------: |
| _aeroplane_ | 0.7887580990791321 |
| _bicycle_ | 0.8351995348930359 |
| _bird_ | 0.7623348236083984 |
| _boat_ | 0.7218425273895264 |
| _bottle_ | 0.45978495478630066 |
| _bus_ | 0.8705356121063232 |
| _car_ | 0.8655831217765808 |
| _cat_ | 0.8828985095024109 |
| _chair_ | 0.5917483568191528 |
| _cow_ | 0.8255912661552429 |
| _diningtable_ | 0.756867527961731 |
| _dog_ | 0.856262743473053 |
| _horse_ | 0.8778411149978638 |
| _motorbike_ | 0.8316892385482788 |
| _person_ | 0.7884440422058105 |
| _pottedplant_ | 0.5071538090705872 |
| _sheep_ | 0.7936667799949646 |
| _sofa_ | 0.7998116612434387 |
| _train_ | 0.8655905723571777 |
| _tvmonitor_ | 0.7492395043373108 |

You can see that some objects, like bottles and potted plants, are considerably harder to detect than others.

# Inference

See [`detect.py`](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/detect.py).

Point to the model you want to use for inference with the `checkpoint` parameter at the beginning of the code.

Then, you can use the `detect()` function to identify and visualize objects in an RGB image.

```python
img_path = '/path/to/ima.ge'
original_image = PIL.Image.open(img_path, mode='r')
original_image = original_image.convert('RGB')

detect(original_image, min_score=0.2, max_overlap=0.5, top_k=200).show()
```

This function first **preprocesses the image by resizing and normalizing its RGB channels** as required by the model. It then **obtains raw predictions from the model, which are parsed** by the `detect_objects()` method in the model. The parsed results are converted from fractional to absolute boundary coordinates, their labels are decoded with the `label_map`, and they are **visualized on the image**.

There are no one-size-fits-all values for `min_score`, `max_overlap`, and `top_k`. You may need to experiment a little to find what works best for your target data.

### Some more examples

---

<p align="center">
<img src="./img/000029.jpg">
</p>

---

<p align="center">
<img src="./img/000045.jpg">
</p>

---

<p align="center">
<img src="./img/000062.jpg">
</p>

---

<p align="center">
<img src="./img/000075.jpg">
</p>

---

<p align="center">
<img src="./img/000085.jpg">
</p>

---

<p align="center">
<img src="./img/000092.jpg">
</p>

---

<p align="center">
<img src="./img/000100.jpg">
</p>

---

<p align="center">
<img src="./img/000124.jpg">
</p>

---

<p align="center">
<img src="./img/000127.jpg">
</p>

---

<p align="center">
<img src="./img/000128.jpg">
</p>

---

<p align="center">
<img src="./img/000145.jpg">
</p>

---

# FAQs

__I noticed that priors often overshoot the `3, 3` kernel employed in the prediction convolutions. How can the kernel detect a bound (of an object) outside it?__

Don't confuse the kernel and its _receptive field_, which is the area of the original image that is represented in the kernel's field-of-view.

For example, on the `38, 38` feature map from `conv4_3`, a `3, 3` kernel covers an area of `0.08, 0.08` in fractional coordinates. The priors are `0.1, 0.1`, `0.14, 0.07`, `0.07, 0.14`, and `0.14, 0.14`.

But its receptive field, which [you can calculate](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807), is a whopping `0.36, 0.36`! Therefore, all priors (and objects contained therein) are present well inside it.

Keep in mind that the receptive field grows with every successive convolution. For `conv_7` and the higher-level feature maps, a `3, 3` kernel's receptive field will cover the _entire_ `300, 300` image. But, as always, the pixels in the original image that are closer to the center of the kernel have greater representation, so it is still _local_ in a sense.

---

__While training, why can't we match predicted boxes directly to their ground truths?__

We cannot directly check for overlap or coincidence between predicted boxes and ground truth objects to match them because predicted boxes are not to be considered reliable, _especially_ during the training process. This is the very reason we are trying to evaluate them in the first place!

And this is why priors are especially useful. We can match a predicted box to a ground truth box by means of the prior it is supposed to be approximating. It no longer matters how correct or wildly wrong the prediction is.

---

__Why do we even have a _background_ class if we're only checking which _non-background_ classes meet the threshold?__

When there is no object in the approximate field of the prior, a high score for _background_ will dilute the scores of the other classes such that they will not meet the detection threshold.

---

__Why not simply choose the class with the highest score instead of using a threshold?__

I think that's a valid strategy. After all, we implicitly conditioned the model to choose _one_ class when we trained it with the Cross Entropy loss. But you will find that you won't achieve the same performance as you would with a threshold.

I suspect this is because object detection is open-ended enough that there's room for doubt in the trained model as to what's really in the field of the prior. For example, the score for _background_ may be high if there is an appreciable amount of backdrop visible in an object's bounding box. There may even be multiple objects present in the same approximate region. A simple threshold will yield all possibilities for our consideration, and it just works better.

Redundant detections aren't really a problem since we're NMS-ing the hell out of 'em.


---

__Sorry, but I gotta ask... _[what's in the boooox?!](https://cnet4.cbsistatic.com/img/cLD5YVGT9pFqx61TuMtcSBtDPyY=/570x0/2017/01/14/6d8103f7-a52d-46de-98d0-56d0e9d79804/se7en.png)___

Ha.
